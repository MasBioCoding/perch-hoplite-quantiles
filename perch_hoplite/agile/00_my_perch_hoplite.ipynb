{"cells":[{"cell_type":"markdown","metadata":{"id":"MVGgeTZLETYe"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-research/perch-hoplite/blob/main/perch_hoplite/agile/1_embed_audio_v2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite/agile/1_embed_audio_v2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2QfDwaWNTZCZ"},"source":["# Overview\n","\n","This notebook uses the latest version, 0.1.3 of the github repo, and thus allowing poetry to install perch-hoplite==1.0.0.dev. Whereas repo 0.1.1 has poetry install perch-hoplite==0.1.1\n","\n","this notebook uses my setup, which uses poetry. It combines both the embeddings and the agile modelling notebooks I pull from the github repo"]},{"cell_type":"code","source":["!git clone https://github.com/google-research/perch-hoplite.git"],"metadata":{"collapsed":true,"id":"B0SchO70jl5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd perch-hoplite\n","!pip install poetry\n","!poetry config virtualenvs.create false"],"metadata":{"collapsed":true,"id":"Wc_0XPlejphM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!poetry install"],"metadata":{"collapsed":true,"id":"8wW19ZcojvRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"YHj7FdlG9853"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTtVnkC-6_i7"},"outputs":[],"source":["# @title Imports\n","\n","from etils import epath\n","from IPython.display import display\n","import ipywidgets as widgets\n","from ml_collections import config_dict\n","import numpy as np\n","\n","from perch_hoplite.agile import colab_utils\n","from perch_hoplite.agile import embed\n","from perch_hoplite.agile import source_info\n","from perch_hoplite.db import brutalism\n","from perch_hoplite.db import interface\n","from perch_hoplite.zoo import taxonomy_model_tf"]},{"cell_type":"markdown","metadata":{"id":"4T4vILrO80iP"},"source":["# Embed the audio data"]},{"cell_type":"markdown","source":["Mount our google drive, and then load the .wav files into the config"],"metadata":{"id":"EqsZfv4aASqI"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NG4vLRKpAalt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content\n","!ls"],"metadata":{"id":"3ETmV45LCNSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6zdGxl68vft"},"outputs":[],"source":["# @title Configuration {vertical-output: true}\n","\n","# @markdown Configure the raw dataset and output location(s).  The format is a mapping from\n","# @markdown a dataset_name to a (base_path, fileglob) pair.  Note that the file\n","# @markdown globs are case sensitive.  The dataset name can be anything you want.\n","#\n","# @markdown This structure allows you to move your data around without having to\n","# @markdown re-embed the dataset.  The generated embedding database will be\n","# @markdown placed in the base path. This allows you to simply swap out\n","# @markdown the base path here if you ever move your dataset.\n","\n","# @markdown By default we only process one dataset at a time.  Re-run this entire notebook\n","# @markdown once per dataset.\n","\n","# @markdown For example, we might set dataset_base_path to `/home/me/myproject`,\n","# @markdown and use the glob `*/*.wav` if all of the audio files have filepaths\n","# @markdown like `/home/me/myproject/site_XYZ/audio_ABC.wav` (e.g. audio files are contained\n","# @markdown in subfolders of the base directory).\n","\n","# @markdown 1. Create a unique name for the database that will store the embeddings for the\n","# @markdown target data.\n","dataset_name = 'deep_sea_data_raw2'  # @param {type: 'string'}\n","\n","# @markdown 2. Input the filepath for the folder that is containing the input audio files.\n","dataset_base_path = '/content/drive/MyDrive/ns_experiment/deep_sea_data_raw2'  # @param {type: 'string'}\n","\n","# @markdown 3. Input the file pattern for the audio files within that folder that you want\n","# @markdown to embed. Some examples for how to input:\n","# @markdown - All files in the base directory of a specific type (not subdirectories): e.g.\n","# @markdown `*.wav` (or `*.flac` etc) will generate embeddings for all .wav files (or whichever\n","# @markdown format) in the `dataset_base_path`.\n","# @markdown - All files in one level of subdirectories within the base directory: `*/*.flac`\n","# @markdown will generate embeddings for all .flac files.\n","# @markdown - Single file: `myfile.wav` will only embed the audio from that specific file.\n","dataset_fileglob = '*/*.wav'  # @param {type:'string'}\n","\n","# @markdown 4. [Optional] If saving the embeddings database to a new directory, specify here.\n","# @markdown Otherwise, leave blank - by default the embeddings database output will be saved\n","# @markdown within `dataset_base_path` where the audio is located. You do not need to specify\n","# @markdown `db_path` unless you want to maintain multiple distinct embedding databases, or\n","# @markdown if you would like to save the output in a different folder. If your input audio\n","# @markdown data is accessed from a public URL, we recommend specifying a separate output\n","# @markdown directory here.\n","db_path = ''  # @param {type:'string'}\n","if not db_path or db_path.lower() == 'none':\n","  db_path = None\n","\n","# @markdown 5. Choose a supported model to generate embeddings. `perch_v2` is the latest Perch\n","# @markdown model and was trained on multiple taxa include birds, mammals, insects and amphibians.\n","# @markdown `perch_v2` has also demonstrated high performance for marine audio transfer learning\n","# @markdown tasks. **NOTE: `perch_v2` only works with GPU runtimes - see above instructions.**\n","# @markdown `perch_8` is the last updated version of Perch V1 trained only on birds, and\n","# @markdown `birdnet_v2.3` is also a common option for birds. Other choices include `surfperch`\n","# @markdown for coral reefs or `multispecies_whale` for marine mammals.\n","model_choice = 'perch_v2'  # @param ['perch_v2', 'perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n","\n","# @markdown 6. [Optional] Shard the audio for embeddings. File sharding automatically splits audio\n","# @markdown files into smaller chunks for creating embeddings. This limits both system and GPU\n","# @markdown memory usage, especially useful when working with long files (>1 hour).\n","use_file_sharding = True  # @param {type:'boolean'}\n","# @markdown If you want to change the length in seconds for the shards, specify here.\n","shard_length_in_seconds = 60  # @param {type:'number'}\n","\n","audio_glob = source_info.AudioSourceConfig(\n","    dataset_name=dataset_name,\n","    base_path=dataset_base_path,\n","    file_glob=dataset_fileglob,\n","    min_audio_len_s=1.0,\n","    target_sample_rate_hz=-2,\n","    shard_len_s=float(shard_length_in_seconds) if use_file_sharding else None,\n",")\n","\n","configs = colab_utils.load_configs(\n","    source_info.AudioSources((audio_glob,)),\n","    db_path,\n","    model_config_key=model_choice,\n","    db_key='sqlite_usearch',\n",")\n","configs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NN9Uyy1yqAWS"},"outputs":[],"source":["# @title Initialize the hoplite database (DB) {vertical-output: true}\n","\n","global db\n","db = configs.db_config.load_db()\n","num_embeddings = db.count_embeddings()\n","\n","print('Initialized DB located at:', configs.db_config.db_config.db_path)\n","\n","def drop_and_reload_db(_) -> interface.HopliteDBInterface:\n","  db_path = epath.Path(configs.db_config.db_config.db_path)\n","  for fp in db_path.glob('hoplite.sqlite*'):\n","    fp.unlink()\n","  (db_path / 'usearch.index').unlink()\n","  print('\\n Deleted previous db at: ', configs.db_config.db_config.db_path)\n","  db = configs.db_config.load_db()\n","\n","# @markdown If `drop_existing_db` set to True, when the database already exists and contains\n","# @markdown embeddings, then those existing embeddings will be erased. You will be prompted\n","# @markdown to confirm you wish to delete those existing embeddings. If you want to keep\n","# @markdown existing embeddings in the database, then set to False, which will append the new\n","# @markdown embeddings to the database.\n","drop_existing_db = True  # @param {type: 'boolean'}\n","\n","if num_embeddings > 0 and drop_existing_db:\n","  print('Existing DB contains projects: ', db.get_all_projects())\n","  print('num embeddings: ', num_embeddings)\n","  print('\\n\\nClick the button below to confirm you really want to drop the database at ')\n","  print(f'{configs.db_config.db_config.db_path}\\n')\n","  print(f'This will permanently delete all {num_embeddings} embeddings from the existing database.\\n')\n","  print('If you do NOT want to delete this data, set `drop_existing_db` above to `False` and re-run this cell.\\n')\n","\n","  button = widgets.Button(description='Delete database?')\n","  button.on_click(drop_and_reload_db)\n","  display(button)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnGWbhc0LhiU"},"outputs":[],"source":["# @title Run the embedding {vertical-output: true}\n","\n","print(f'Embedding dataset as a new db project: {audio_glob.dataset_name}')\n","\n","worker = embed.EmbedWorker(\n","    audio_sources=configs.audio_sources_config,\n","    db=db,\n","    model_config=configs.model_config,\n",")\n","\n","worker.process_all(target_dataset_name=audio_glob.dataset_name)\n","\n","print('\\n\\nEmbedding complete, total embeddings: ', db.count_embeddings())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvVuFw-somHe"},"outputs":[],"source":["# @title Per project statistics {vertical-output: true}\n","\n","for project in db.get_all_projects():\n","  window_ids = db.match_window_ids(\n","      deployments_filter=config_dict.create(eq=dict(project=project))\n","  )\n","  print('Project:', project)\n","  print('>>> num embeddings:', len(window_ids))\n","  print()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ihBNRbwuuwal"},"outputs":[],"source":["# @title Show example embedding search\n","# @markdown As an example (and to show that the embedding process worked), this selects a single\n","# @markdown embedding from the database and outputs the embedding ids of the top-k (k = 128)\n","# @markdown nearest neighbors in the database.\n","\n","q = db.get_embedding(db.match_window_ids(limit=1)[0])\n","%time results, scores = brutalism.brute_search(worker.db, query_embedding=q, search_list_size=128, score_fn=np.dot)\n","print([int(r.window_id) for r in results])"]},{"cell_type":"markdown","source":["Here the agile modeling starts"],"metadata":{"id":"9fJZDYVTNDHE"}},{"cell_type":"code","source":["# @title Imports\n","\n","import os\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","from perch_hoplite.agile import audio_loader\n","from perch_hoplite.agile import classifier\n","from perch_hoplite.agile import classifier_data\n","from perch_hoplite.agile import embedding_display\n","from perch_hoplite.agile import source_info\n","from perch_hoplite.db  import brutalism\n","from perch_hoplite.db import score_functions\n","from perch_hoplite.db  import search_results\n","from perch_hoplite.db import sqlite_usearch_impl\n","from perch_hoplite.zoo import model_configs\n","from perch_hoplite.zoo import taxonomy_model_tf"],"metadata":{"id":"OIMp-I2mrko8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Load model and connect to database {vertical-output: true}\n","\n","# @markdown Location of database containing audio embeddings.\n","db_path = '/content/drive/MyDrive/ns_experiment/deep_sea_data_raw2'  # @param {type: 'string'}\n","\n","# @markdown Identifier (e.g. name) to attach to labels produced during validation.\n","annotator_id = 'Mas'  # @param {type: 'string'}\n","\n","db = sqlite_usearch_impl.SQLiteUSearchDB.create(db_path)\n","db_model_config = db.get_metadata('model_config')\n","embed_config = db.get_metadata('audio_sources')\n","model_class = model_configs.get_model_class(db_model_config.model_key)\n","embedding_model = model_class.from_config(db_model_config.model_config)\n","audio_sources = source_info.AudioSources.from_config_dict(embed_config)\n","if hasattr(embedding_model, 'window_size_s'):\n","  window_size_s = embedding_model.window_size_s\n","else:\n","  window_size_s = 5.0\n","audio_filepath_loader = audio_loader.make_filepath_loader(\n","    audio_sources=audio_sources,\n","    window_size_s=window_size_s,\n","    sample_rate_hz=embedding_model.sample_rate,\n",")"],"metadata":{"id":"iTwpyzCMrrM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Load query audio {vertical-output: true}\n","\n","# @markdown The `query_uri` can be a URL, filepath, or Xeno-Canto ID\n","# @markdown (like `xc1038305`, containing an Eastern Towhee (`eastow`)).\n","query_uri = '/content/drive/MyDrive/ns_experiment/deep_sea_data_raw2/ZOOM0003/clip_001695.wav'  # @param {type: 'string'}\n","query_label = 'fish'  # @param {type: 'string'}\n","\n","query = embedding_display.QueryDisplay(\n","    uri=query_uri, offset_s=0.0, window_size_s=5.0, sample_rate_hz=32000)\n","_ = query.display_interactive()"],"metadata":{"id":"wCKHmZ5tr048"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Embed the Query and Search {vertical-output: true}\n","\n","# @markdown Number of results to find and display.\n","num_results = 50  # @param\n","query_embedding = embedding_model.embed(\n","    query.get_audio_window()).embeddings[0, 0]\n","\n","# @markdown If checked, search for examples near a particular target score.\n","target_sampling = False  # @param {type: 'boolean'}\n","\n","# @markdown When target sampling, target this score.\n","target_score = -1.0  # @param\n","if not target_sampling:\n","  target_score = None\n","\n","# @markdown If True, search the full DB. Otherwise, use approximate\n","# @markdown nearest-neighbor search. (Mas comment: i believe false is usefull\n","# @markdown for very large datasets, e.g. >100k embeds, check conclusion\n","# @markdown in paper stating recall might be lower?\n","exact_search = True  # @param {type: 'boolean'}\n","\n","if exact_search:\n","  score_fn = score_functions.get_score_fn('dot', target_score=target_score)\n","  results, all_scores = brutalism.threaded_brute_search(\n","      db, query_embedding, num_results, score_fn=score_fn)\n","  if not target_sampling:\n","    # TODO(tomdenton): Create a reasonable histogram when target sampling.\n","    all_scores = [s for s in all_scores if s == target_score]\n","    _ = plt.hist(all_scores, bins=100)\n","    hit_scores = [r.sort_score for r in results.search_results]\n","    plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n","                color='r', alpha=0.5)\n","else:\n","  ann_matches = db.ui.search(query_embedding, count=num_results)\n","  results = search_results.TopKSearchResults(top_k=num_results)\n","  for k, d in zip(ann_matches.keys, ann_matches.distances):\n","    results.update(search_results.SearchResult(k, d))\n"],"metadata":{"id":"NeE9YXRLr5dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Display Results {vertical-output: true}\n","\n","display_results = embedding_display.EmbeddingDisplayGroup.from_search_results(\n","    results, db, sample_rate_hz=32000, frame_rate=100,\n","    audio_loader=audio_filepath_loader)\n","display_results.display(positive_labels=[query_label])"],"metadata":{"id":"EnVNFwAJsCwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Save data labels {vertical-output: true}\n","\n","print(\"Annotations before saving new labels:\", len(db.get_all_annotations()))\n","\n","for ann in display_results.harvest_labels(annotator_id):\n","  db.insert_annotation(\n","      window_id=ann.window_id,\n","      label=ann.label,\n","      label_type=ann.label_type,\n","      provenance=ann.provenance,\n","      skip_duplicates=True,\n","  )\n","\n","print(\"Annotations after saving new labels:\", len(db.get_all_annotations()))"],"metadata":{"id":"kv2hHxsKsSra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Classifier training {vertical-output: true}\n","\n","# @markdown Set of labels to classify. If None, auto-populated from the DB.\n","target_labels = None  # @param\n","\n","# @markdown Classifier traning hyperparams. These should not require tuning.\n","learning_rate = 1e-3  # @param\n","weak_neg_weight = 0.05  # @param\n","l2_mu = 0.000  # @param\n","num_steps = 128  # @param\n","\n","train_ratio = 0.9  # @param\n","batch_size = 128  # @param\n","weak_negatives_batch_size = 128  # @param\n","loss_fn_name = 'bce'  # @param ['hinge', 'bce']\n","\n","data_manager = classifier_data.AgileDataManager(\n","    target_labels=target_labels,\n","    db=db,\n","    train_ratio=train_ratio,\n","    min_eval_examples=1,\n","    batch_size=batch_size,\n","    weak_negatives_batch_size=weak_negatives_batch_size,\n","    rng=np.random.default_rng(seed=5))\n","print('Training for target labels : ')\n","print(data_manager.get_target_labels())\n","linear_classifier, eval_scores = classifier.train_linear_classifier(\n","    data_manager=data_manager,\n","    learning_rate=learning_rate,\n","    weak_neg_weight=weak_neg_weight,\n","    num_train_steps=num_steps,\n",")\n","print('\\n' + '-' * 80)\n","top1 = eval_scores['top1_acc']\n","print(f'top-1      {top1:.3f}')\n","rocauc = eval_scores['roc_auc']\n","print(f'roc_auc    {rocauc:.3f}')\n","cmap = eval_scores['cmap']\n","print(f'cmap       {cmap:.3f}')\n","\n","# Save linear classifier.\n","linear_classifier.save(os.path.join(db_path, 'agile_classifier_v2.pt'))"],"metadata":{"id":"xmEnKjE2sg1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Review Classifier Results {vertical-output: true}\n","\n","# @markdown Number of results to find and display.\n","target_label = 'fish'  # @param {type: 'string'}\n","num_results = 50  # @param\n","\n","target_label_idx = data_manager.get_target_labels().index(target_label)\n","class_query = linear_classifier.beta[:, target_label_idx]\n","bias = linear_classifier.beta_bias[target_label_idx]\n","\n","# @markdown Number of (randomly selected) database entries to search over.\n","sample_size = 1_000_000  # @param\n","\n","# @markdown Whether to use margin-sampling. If checked, search for examples\n","# @markdown with logits near a particular target score (usually 0).\n","margin_sampling = False  # @param {type: 'boolean'}\n","\n","# @markdown When margin sampling, target this logit.\n","margin_target_score = -0.0  # @param\n","if not margin_sampling:\n","  margin_target_score = None\n","score_fn = score_functions.get_score_fn(\n","    'dot', bias=bias, target_score=margin_target_score)\n","results, all_scores = brutalism.threaded_brute_search(\n","    db, class_query, num_results, score_fn=score_fn,\n","    sample_size=sample_size)\n","\n","if not margin_sampling:\n","  # TODO(tomdenton): Create a reasonable histogram when margin sampling.\n","  _ = plt.hist(all_scores, bins=100)\n","  hit_scores = [r.sort_score for r in results.search_results]\n","  plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n","              color='r', alpha=0.5)\n"],"metadata":{"id":"zajHFBnosn-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Display Results {vertical-output: true}\n","\n","display_results = embedding_display.EmbeddingDisplayGroup.from_search_results(\n","    results, db, sample_rate_hz=32000, frame_rate=100,\n","    audio_loader=audio_filepath_loader)\n","display_results.display(positive_labels=[target_label])"],"metadata":{"id":"7C-5Ztk_syj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Save data labels {vertical-output: true}\n","\n","print(\"Annotations before saving new labels:\", len(db.get_all_annotations()))\n","\n","for ann in display_results.harvest_labels(annotator_id):\n","  db.insert_annotation(\n","      window_id=ann.window_id,\n","      label=ann.label,\n","      label_type=ann.label_type,\n","      provenance=ann.provenance,\n","      skip_duplicates=True,\n","  )\n","\n","print(\"Annotations after saving new labels:\", len(db.get_all_annotations()))"],"metadata":{"id":"yhIny27UtBGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Run inference with trained classifier {vertical-output: true}\n","\n","output_csv_filepath = '/content/drive/MyDrive/ns_experiment/deep_sea_data_raw2/inference.csv'  # @param {type: 'string'}\n","logit_threshold = 1.0  # @param\n","# Set labels to a tuple of desired labels if you want to run inference on\n","# a subset of the labels.\n","labels = None  # @param\n","\n","classifier.write_inference_csv(\n","    linear_classifier,\n","    db,\n","    output_csv_filepath,\n","    logit_threshold,\n","    labels=labels,\n","    window_ids=db.match_window_ids(),\n",")"],"metadata":{"id":"7xV7If6UtPOv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"last_runtime":{"build_target":"//third_party/py/chirp:colab_binary","kind":"private"},"private_outputs":true,"provenance":[{"file_id":"1ePT3-fDB3kA3_T7trthFtu8xTJQWQBoQ","timestamp":1723499538314}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}